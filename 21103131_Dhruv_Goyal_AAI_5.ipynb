{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "227fd76e",
   "metadata": {},
   "source": [
    "# Network Traffic Signal Control using Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e015d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62582e5d",
   "metadata": {},
   "source": [
    "### Simulate the Traffic Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e7c5c536",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrafficEnvironment:\n",
    "    def __init__(self, num_intersections):\n",
    "        self.num_intersections = num_intersections\n",
    "        self.max_cars = 20  # Maximum number of cars at each intersection\n",
    "        self.state = np.zeros(num_intersections, dtype=int)\n",
    "        \n",
    "    def reset(self): # resets the cuurent state to get the randomness\n",
    "        self.state = np.random.randint(0, self.max_cars + 1, self.num_intersections)\n",
    "        return tuple(self.state)  # Return state as a tuple\n",
    "    \n",
    "    def step(self, action):\n",
    "        reward = 0\n",
    "        \n",
    "        # Applying the action (change in traffic lights)\n",
    "        for i in range(self.num_intersections):\n",
    "            if action[i] == 1:  # Green light\n",
    "                cars_passed = min(self.state[i], 5)  # Assuming max 5 cars can pass on green\n",
    "                self.state[i] -= cars_passed\n",
    "                reward -= self.state[i]  # Negative reward for waiting cars\n",
    "            else:  # Red light\n",
    "                self.state[i] = min(self.state[i] + np.random.randint(0, 3), self.max_cars)\n",
    "                reward -= self.state[i] * 2  # Higher negative reward for red light\n",
    "        \n",
    "        # Adding some randomness to car arrivals\n",
    "        self.state += np.random.randint(0, 3, self.num_intersections)\n",
    "        self.state = np.clip(self.state, 0, self.max_cars)\n",
    "        \n",
    "        return tuple(self.state), reward, False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8916077c",
   "metadata": {},
   "source": [
    "### Implementing Q-Learning Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "46949039",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningAgent:\n",
    "    def __init__(self, num_intersections, learning_rate=0.05, discount_factor=0.99, epsilon=0.1):\n",
    "        self.num_intersections = num_intersections\n",
    "        self.num_actions = 2 ** num_intersections\n",
    "        self.q_table = {}\n",
    "        self.lr = learning_rate\n",
    "        self.gamma = discount_factor\n",
    "        self.epsilon = epsilon\n",
    "    \n",
    "    def get_action(self, state):\n",
    "        if state not in self.q_table:\n",
    "            self.q_table[state] = np.zeros(self.num_actions)\n",
    "        \n",
    "        if random.random() < self.epsilon:\n",
    "            return random.randint(0, self.num_actions - 1) # Exploration \n",
    "        else:\n",
    "            return np.argmax(self.q_table[state]) # Exploitation\n",
    "    \n",
    "    def update_q_table(self, state, action, reward, next_state):\n",
    "        if state not in self.q_table:\n",
    "            self.q_table[state] = np.zeros(self.num_actions)\n",
    "        \n",
    "        if next_state not in self.q_table:\n",
    "            self.q_table[next_state] = np.zeros(self.num_actions)\n",
    "        \n",
    "        current_q = self.q_table[state][action]\n",
    "        max_next_q = np.max(self.q_table[next_state])\n",
    "        new_q = current_q + self.lr * (reward + self.gamma * max_next_q - current_q)\n",
    "        self.q_table[state][action] = new_q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39567383",
   "metadata": {},
   "source": [
    "### Training the Q-Learning Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d03e43a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(num_episodes, num_intersections):\n",
    "    env = TrafficEnvironment(num_intersections)\n",
    "    agent = QLearningAgent(num_intersections)\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        \n",
    "        for _ in range(100):  # 100 time steps per episode\n",
    "            action = agent.get_action(state)\n",
    "            action_binary = [int(x) for x in format(action, f'0{num_intersections}b')]\n",
    "            next_state, reward, done = env.step(action_binary)\n",
    "            agent.update_q_table(state, action, reward, next_state)\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "        \n",
    "        if episode % 100 == 0:\n",
    "            print(f\"Episode {episode}, Total Reward: {total_reward}\")\n",
    "    \n",
    "    return agent\n",
    "\n",
    "# Train the agent\n",
    "num_intersections = 4\n",
    "num_episodes = 10000\n",
    "trained_agent = train(num_episodes, num_intersections)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5704af02",
   "metadata": {},
   "source": [
    "### Testing the Trained Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b7c2a10f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0, Total Reward: -13428\n",
      "Episode 100, Total Reward: -10090\n",
      "Episode 200, Total Reward: -9556\n",
      "Episode 300, Total Reward: -9064\n",
      "Episode 400, Total Reward: -9029\n",
      "Episode 500, Total Reward: -9109\n",
      "Episode 600, Total Reward: -7364\n",
      "Episode 700, Total Reward: -7283\n",
      "Episode 800, Total Reward: -7777\n",
      "Episode 900, Total Reward: -7634\n",
      "Episode 1000, Total Reward: -8178\n",
      "Episode 1100, Total Reward: -7694\n",
      "Episode 1200, Total Reward: -7273\n",
      "Episode 1300, Total Reward: -8466\n",
      "Episode 1400, Total Reward: -7783\n",
      "Episode 1500, Total Reward: -7857\n",
      "Episode 1600, Total Reward: -6463\n",
      "Episode 1700, Total Reward: -8023\n",
      "Episode 1800, Total Reward: -7498\n",
      "Episode 1900, Total Reward: -7955\n",
      "Episode 2000, Total Reward: -7168\n",
      "Episode 2100, Total Reward: -6354\n",
      "Episode 2200, Total Reward: -6973\n",
      "Episode 2300, Total Reward: -6872\n",
      "Episode 2400, Total Reward: -6259\n",
      "Episode 2500, Total Reward: -6723\n",
      "Episode 2600, Total Reward: -6170\n",
      "Episode 2700, Total Reward: -4949\n",
      "Episode 2800, Total Reward: -6399\n",
      "Episode 2900, Total Reward: -6597\n",
      "Episode 3000, Total Reward: -5579\n",
      "Episode 3100, Total Reward: -5422\n",
      "Episode 3200, Total Reward: -5102\n",
      "Episode 3300, Total Reward: -5193\n",
      "Episode 3400, Total Reward: -5462\n",
      "Episode 3500, Total Reward: -5639\n",
      "Episode 3600, Total Reward: -5814\n",
      "Episode 3700, Total Reward: -5560\n",
      "Episode 3800, Total Reward: -5466\n",
      "Episode 3900, Total Reward: -4829\n",
      "Episode 4000, Total Reward: -4479\n",
      "Episode 4100, Total Reward: -5020\n",
      "Episode 4200, Total Reward: -4511\n",
      "Episode 4300, Total Reward: -5178\n",
      "Episode 4400, Total Reward: -4852\n",
      "Episode 4500, Total Reward: -8731\n",
      "Episode 4600, Total Reward: -5692\n",
      "Episode 4700, Total Reward: -5244\n",
      "Episode 4800, Total Reward: -4762\n",
      "Episode 4900, Total Reward: -5159\n",
      "Episode 5000, Total Reward: -4829\n",
      "Episode 5100, Total Reward: -4951\n",
      "Episode 5200, Total Reward: -4022\n",
      "Episode 5300, Total Reward: -3977\n",
      "Episode 5400, Total Reward: -3921\n",
      "Episode 5500, Total Reward: -5091\n",
      "Episode 5600, Total Reward: -4193\n",
      "Episode 5700, Total Reward: -3682\n",
      "Episode 5800, Total Reward: -1884\n",
      "Episode 5900, Total Reward: -1966\n",
      "Episode 6000, Total Reward: -3349\n",
      "Episode 6100, Total Reward: -3720\n",
      "Episode 6200, Total Reward: -5092\n",
      "Episode 6300, Total Reward: -3332\n",
      "Episode 6400, Total Reward: -1971\n",
      "Episode 6500, Total Reward: -3225\n",
      "Episode 6600, Total Reward: -2288\n",
      "Episode 6700, Total Reward: -3192\n",
      "Episode 6800, Total Reward: -1960\n",
      "Episode 6900, Total Reward: -3755\n",
      "Episode 7000, Total Reward: -3528\n",
      "Episode 7100, Total Reward: -5357\n",
      "Episode 7200, Total Reward: -3687\n",
      "Episode 7300, Total Reward: -1767\n",
      "Episode 7400, Total Reward: -501\n",
      "Episode 7500, Total Reward: -3104\n",
      "Episode 7600, Total Reward: -471\n",
      "Episode 7700, Total Reward: -1204\n",
      "Episode 7800, Total Reward: -4031\n",
      "Episode 7900, Total Reward: -3616\n",
      "Episode 8000, Total Reward: -794\n",
      "Episode 8100, Total Reward: -2055\n",
      "Episode 8200, Total Reward: -4327\n",
      "Episode 8300, Total Reward: -1604\n",
      "Episode 8400, Total Reward: -752\n",
      "Episode 8500, Total Reward: -1410\n",
      "Episode 8600, Total Reward: -2185\n",
      "Episode 8700, Total Reward: -4428\n",
      "Episode 8800, Total Reward: -2872\n",
      "Episode 8900, Total Reward: -3695\n",
      "Episode 9000, Total Reward: -5308\n",
      "Episode 9100, Total Reward: -1835\n",
      "Episode 9200, Total Reward: -623\n",
      "Episode 9300, Total Reward: -653\n",
      "Episode 9400, Total Reward: -2341\n",
      "Episode 9500, Total Reward: -550\n",
      "Episode 9600, Total Reward: -2113\n",
      "Episode 9700, Total Reward: -1519\n",
      "Episode 9800, Total Reward: -2527\n",
      "Episode 9900, Total Reward: -742\n",
      "Test Total Reward: -1665\n"
     ]
    }
   ],
   "source": [
    "env = TrafficEnvironment(num_intersections)\n",
    "state = env.reset()\n",
    "total_reward = 0\n",
    "\n",
    "for _ in range(100):\n",
    "    action = trained_agent.get_action(state)\n",
    "    action_binary = [int(x) for x in format(action, f'0{num_intersections}b')]\n",
    "    next_state, reward, _ = env.step(action_binary)\n",
    "    state = next_state\n",
    "    total_reward += reward\n",
    "\n",
    "print(f\"Test Total Reward: {total_reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fee4e6c",
   "metadata": {},
   "source": [
    "* Penaltes (negative reward) are decreasing overtime, which means agent is learning"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
